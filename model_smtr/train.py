import torch
import torch.utils.data
import torch.backends.cudnn as cudnn
from torch.utils.tensorboard import SummaryWriter

import os
import json
import valid
from utils import utils
from utils import sam
from utils import option
from data import dataset
from model import HTR_VT
from model.SMTR import build_smtr_vocab, make_smtr_batch, SMTRHead
from functools import partial
import random
import numpy as np
import re
import importlib


def compute_loss(args, model, image, batch_size, criterion_ctc, text, length,
                 smtr_on=False, smtr_head=None, smtr_cfg=None, labels_for_smtr=None):
    """
    Returns total loss = CTC + lambda * SMTR (if enabled)
    Uses model.extract_features once to avoid recomputation.
    """
    # ---- features and logits (one encoder pass) ----
    feats = model.extract_features(
        # (B,L,D)
        image, args.mask_ratio, args.max_span_length, use_masking=True)
    # (B,L,V_ctc)
    logits = model.head(feats)
    # keep your existing final LN
    logits = model.layer_norm(logits)

    # ---- CTC loss ----
    preds_size = torch.IntTensor([logits.size(1)] * batch_size).cuda()
    ctc_log_probs = logits.permute(
        1, 0, 2).log_softmax(2)             # (T,B,C)
    torch.backends.cudnn.enabled = False
    ctc_loss = criterion_ctc(ctc_log_probs, text.cuda(),
                             preds_size, length.cuda()).mean()
    torch.backends.cudnn.enabled = True

    total_loss = ctc_loss

    # ---- SMTR loss (optional) ----
    if smtr_on and smtr_head is not None and smtr_cfg is not None and labels_for_smtr is not None:
        # build SMTR batch on the fly (CPU numpy, then move to CUDA)
        substr, tgt_n, tgt_p = make_smtr_batch(
            labels_for_smtr,
            smtr_cfg["stoi"],
            ls=smtr_cfg["ls"],
            max_substrings=smtr_cfg["S"],
            rand_replace_p=smtr_cfg["rand_p"]
        )
        substr = substr.cuda(non_blocking=True)
        tgt_n = tgt_n.cuda(non_blocking=True)
        tgt_p = tgt_p.cuda(non_blocking=True)

        # logits from SMTR head: (B,S,Vsmtr)
        logit_next, logit_prev = smtr_head(feats, substr)
        # Cross-entropy over the vocab (includes [B] and [E])
        ce = torch.nn.CrossEntropyLoss(reduction='none')
        loss_next = ce(
            logit_next.view(-1, logit_next.size(-1)), tgt_n.view(-1))
        loss_prev = ce(
            logit_prev.view(-1, logit_prev.size(-1)), tgt_p.view(-1))
        smtr_loss = (loss_next + loss_prev).mean()

        total_loss = total_loss + smtr_cfg["lambda"] * smtr_loss
    else:
        smtr_loss = torch.tensor(0.0, device=image.device)

    return total_loss, ctc_loss.detach(), smtr_loss.detach()


def main():

    args = option.get_args_parser()
    torch.manual_seed(args.seed)

    args.save_dir = os.path.join(args.out_dir, args.exp_name)
    os.makedirs(args.save_dir, exist_ok=True)

    logger = utils.get_logger(args.save_dir)
    logger.info(json.dumps(vars(args), indent=4, sort_keys=True))
    writer = SummaryWriter(args.save_dir)

    # Initialize wandb only if enabled
    if getattr(args, 'use_wandb', False):
        try:
            wandb = importlib.import_module('wandb')
            wandb.init(project=getattr(args, 'wandb_project', 'None'), name=args.exp_name,
                       config=vars(args), dir=args.save_dir)
            logger.info("Weights & Biases logging enabled")
        except Exception as e:
            logger.warning(
                f"Failed to initialize wandb: {e}. Continuing without wandb.")
            wandb = None
    else:
        wandb = None

    model = HTR_VT.create_model(
        nb_cls=args.nb_cls, img_size=args.img_size[::-1])

    total_param = sum(p.numel() for p in model.parameters())
    logger.info('total_param is {}'.format(total_param))

    model.train()
    model = model.cuda()

    ema_decay = getattr(args, 'ema_decay', 0.9999)
    logger.info(f"Using EMA decay: {ema_decay}")
    model_ema = utils.ModelEma(model, ema_decay)
    model.zero_grad()

    # Use centralized checkpoint loader like model_v4-2
    resume_path = args.resume if getattr(
        args, 'resume', None) else getattr(args, 'resume_checkpoint', None)
    best_cer, best_wer, start_iter, optimizer_state, train_loss, train_loss_count = utils.load_checkpoint(
        model, model_ema, None, resume_path, logger)

    logger.info('Loading train loader...')
    train_dataset = dataset.myLoadDS(
        args.train_data_list, args.data_path, args.img_size)
    train_loader = torch.utils.data.DataLoader(train_dataset,
                                               batch_size=args.train_bs,
                                               shuffle=True,
                                               pin_memory=True,
                                               num_workers=args.num_workers,
                                               collate_fn=partial(dataset.SameTrCollate, args=args))
    train_iter = dataset.cycle_data(train_loader)

    logger.info('Loading val loader...')
    val_dataset = dataset.myLoadDS(
        args.val_data_list, args.data_path, args.img_size, ralph=train_dataset.ralph)
    val_loader = torch.utils.data.DataLoader(val_dataset,
                                             batch_size=args.val_bs,
                                             shuffle=False,
                                             pin_memory=True,
                                             num_workers=args.num_workers)

    criterion = torch.nn.CTCLoss(reduction='none', zero_infinity=True)
    converter = utils.CTCLabelConverter(train_dataset.ralph.values())

    # === SMTR setup now that we know the alphabet ===
    # default ON for your experiment
    smtr_on = getattr(args, "smtr_on", True)
    smtr_ls = getattr(args, "smtr_ls", 5)
    smtr_S = getattr(args, "smtr_samples", 24)
    smtr_lambda = getattr(args, "smtr_lambda", 0.3)
    smtr_rand_p = getattr(args, "smtr_rand_replace_p", 0.3)

    smtr_itos, smtr_stoi = build_smtr_vocab(converter.dict.keys())
    smtr_vocab_size = len(smtr_itos)
    # grab d_model from the Linear head in HTR-VT (in_features)
    d_model = model.head.in_features
    smtr_head = SMTRHead(d_model, smtr_vocab_size,
                         dropout=0.1).cuda() if smtr_on else None

    # ---- optimizer: include SMTR params if enabled ----
    all_params = list(model.parameters()) + \
        (list(smtr_head.parameters()) if smtr_on else [])
    optimizer = sam.SAM(all_params, torch.optim.AdamW, lr=1e-7,
                        betas=(0.9, 0.99), weight_decay=args.weight_decay)
    # === /SMTR === #

    # Load optimizer state after initialization
    if optimizer_state is not None:
        try:
            optimizer.load_state_dict(optimizer_state)
            logger.info("Successfully loaded optimizer state")
        except Exception as e:
            logger.warning(f"Failed to load optimizer state: {e}")
            logger.info(
                "Continuing training without optimizer state (will restart from initial lr/momentum)")

    best_cer, best_wer = best_cer, best_wer
    train_loss = train_loss
    train_loss_count = train_loss_count

    #### ---- train & eval ---- ####
    logger.info('Start training...')
    for nb_iter in range(start_iter, args.total_iter):

        optimizer, current_lr = utils.update_lr_cos(
            nb_iter, args.warm_up_iter, args.total_iter, args.max_lr, optimizer)

        optimizer.zero_grad()
        batch = next(train_iter)
        image = batch[0].cuda()
        text, length = converter.encode(batch[1])
        batch_size = image.size(0)

        # ---- SMTR loss ---- #
        smtr_cfg = None
        if smtr_on:
            smtr_cfg = {"stoi": smtr_stoi, "ls": smtr_ls, "S": smtr_S,
                        "rand_p": smtr_rand_p, "lambda": smtr_lambda}

        loss, ctc_loss_val, smtr_loss_val = compute_loss(
            args, model, image, batch_size, criterion, text, length,
            smtr_on=smtr_on, smtr_head=smtr_head, smtr_cfg=smtr_cfg,
            labels_for_smtr=batch[1]  # raw strings
        )
        loss.backward()
        optimizer.first_step(zero_grad=True)

        loss, ctc_loss_val, smtr_loss_val = compute_loss(
            args, model, image, batch_size, criterion, text, length,
            smtr_on=smtr_on, smtr_head=smtr_head, smtr_cfg=smtr_cfg,
            labels_for_smtr=batch[1]
        )
        loss.backward()
        optimizer.second_step(zero_grad=True)
        # --- /SMTR ---- #

        model.zero_grad()
        model_ema.update(model, num_updates=nb_iter / 2)
        train_loss += loss.item()
        train_loss_count += 1

        if nb_iter % args.print_iter == 0:
            train_loss_avg = train_loss / train_loss_count if train_loss_count > 0 else 0.0

            logger.info(f'Iter : {nb_iter} \t LR : {current_lr:0.5f} \t '
                        f'CTC: {ctc_loss_val.item():0.5f} \t SMTR: {smtr_loss_val.item():0.5f}')

            writer.add_scalar('./Train/lr', current_lr, nb_iter)
            writer.add_scalar('./Train/train_loss', train_loss_avg, nb_iter)
            if wandb is not None:
                wandb.log({
                    'train/lr': current_lr,
                    'train/ctc_loss': ctc_loss_val.item(),
                    'train/smtr_loss': smtr_loss_val.item(),
                    'iter': nb_iter,
                }, step=nb_iter)
            train_loss = 0.0
            train_loss_count = 0

        if nb_iter % args.eval_iter == 0:
            model.eval()
            with torch.no_grad():
                val_loss, val_cer, val_wer, preds, labels = valid.validation(model_ema.ema,
                                                                             criterion,
                                                                             val_loader,
                                                                             converter)
                # Save checkpoint every print interval (like model_v4-2)
                ckpt_name = f"checkpoint_{best_cer:.4f}_{best_wer:.4f}_{nb_iter}.pth"
                checkpoint = {
                    'model': model.state_dict(),
                    'state_dict_ema': model_ema.ema.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'nb_iter': nb_iter,
                    'best_cer': best_cer,
                    'best_wer': best_wer,
                    'args': vars(args),
                    'random_state': random.getstate(),
                    'numpy_state': np.random.get_state(),
                    'torch_state': torch.get_rng_state(),
                    'torch_cuda_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None,
                    'train_loss': train_loss,
                    'train_loss_count': train_loss_count,
                }
                torch.save(checkpoint, os.path.join(args.save_dir, ckpt_name))
                if val_cer < best_cer:
                    logger.info(
                        f'CER improved from {best_cer:.4f} to {val_cer:.4f}!!!')
                    best_cer = val_cer
                    checkpoint = {
                        'model': model.state_dict(),
                        'state_dict_ema': model_ema.ema.state_dict(),
                        'optimizer': optimizer.state_dict(),
                        'nb_iter': nb_iter,
                        'best_cer': best_cer,
                        'best_wer': best_wer,
                        'args': vars(args),
                        'random_state': random.getstate(),
                        'numpy_state': np.random.get_state(),
                        'torch_state': torch.get_rng_state(),
                        'torch_cuda_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None,
                        'train_loss': train_loss,
                        'train_loss_count': train_loss_count,
                    }
                    torch.save(checkpoint, os.path.join(
                        args.save_dir, 'best_CER.pth'))

                if val_wer < best_wer:
                    logger.info(
                        f'WER improved from {best_wer:.4f} to {val_wer:.4f}!!!')
                    best_wer = val_wer
                    checkpoint = {
                        'model': model.state_dict(),
                        'state_dict_ema': model_ema.ema.state_dict(),
                        'optimizer': optimizer.state_dict(),
                        'nb_iter': nb_iter,
                        'best_cer': best_cer,
                        'best_wer': best_wer,
                        'args': vars(args),
                        'random_state': random.getstate(),
                        'numpy_state': np.random.get_state(),
                        'torch_state': torch.get_rng_state(),
                        'torch_cuda_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None,
                        'train_loss': train_loss,
                        'train_loss_count': train_loss_count,
                    }
                    torch.save(checkpoint, os.path.join(
                        args.save_dir, 'best_WER.pth'))

                logger.info(
                    f'Val. loss : {val_loss:0.3f} \t CER : {val_cer:0.4f} \t WER : {val_wer:0.4f} \t ')

                writer.add_scalar('./VAL/CER', val_cer, nb_iter)
                writer.add_scalar('./VAL/WER', val_wer, nb_iter)
                writer.add_scalar('./VAL/bestCER', best_cer, nb_iter)
                writer.add_scalar('./VAL/bestWER', best_wer, nb_iter)
                writer.add_scalar('./VAL/val_loss', val_loss, nb_iter)
                if wandb is not None:
                    wandb.log({
                        'val/loss': val_loss,
                        'val/CER': val_cer,
                        'val/WER': val_wer,
                        'val/best_CER': best_cer,
                        'val/best_WER': best_wer,
                        'iter': nb_iter,
                    }, step=nb_iter)
                model.train()


if __name__ == '__main__':
    main()
