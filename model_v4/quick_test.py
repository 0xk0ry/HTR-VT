import torch
import torch.nn as nn
import numpy as np
import argparse
from model import HTR_VT
from data.dataset import decompose_vietnamese, merge_base_and_diacritic
from utils import utils

def create_dummy_data(batch_size=2, img_height=64, img_width=512, seq_len=10):
    """Create dummy data for testing"""
    # Create dummy images
    images = torch.randn(batch_size, 1, img_height, img_width)
    
    # Create dummy Vietnamese text samples
    sample_texts = ["c√°c", "ch√†o", "xin", "t√¥i", "vi·ªát"]
    
    base_labels = []
    diacritic_labels = []
    
    for i in range(batch_size):
        # Pick a random text
        text = sample_texts[i % len(sample_texts)]
        base_seq, diac_seq = decompose_vietnamese(text)
        base_labels.append(base_seq)
        diacritic_labels.append(diac_seq)
    
    return images, base_labels, diacritic_labels

def test_model_forward():
    """Test basic forward pass"""
    print("üîç Testing model forward pass...")
    
    # Create model
    nb_cls = 84  # Number of base character classes
    img_size = [512, 64]
    model = HTR_VT.create_model(nb_cls=nb_cls, img_size=img_size[::-1])
    
    # Create dummy data
    batch_size = 2
    images, base_labels, diacritic_labels = create_dummy_data(batch_size)
    
    print(f"Input shape: {images.shape}")
    print(f"Base labels: {base_labels}")
    print(f"Diacritic labels: {diacritic_labels}")
    
    # Test forward pass
    try:
        model.eval()
        with torch.no_grad():
            base_logits, diacritic_logits = model(images)
            
        print(f"‚úÖ Forward pass successful!")
        print(f"Base logits shape: {base_logits.shape}")
        print(f"Diacritic logits shape: {diacritic_logits.shape}")
        print(f"Expected base shape: (B={batch_size}, T=?, nb_cls={nb_cls})")
        print(f"Expected diacritic shape: (B={batch_size}, T=?, 6)")
        
        return True, model, images, base_labels, diacritic_labels
        
    except Exception as e:
        print(f"‚ùå Forward pass failed: {e}")
        return False, None, None, None, None

def test_loss_computation(model, images, base_labels, diacritic_labels):
    """Test loss computation with dummy args"""
    print("\nüîç Testing loss computation...")
    
    # Create dummy args
    class DummyArgs:
        mask_ratio = 0.0
        max_span_length = 1
        alpha_ctc = 1.0
        alpha_diac = 1.0
    
    args = DummyArgs()
    
    # Create base alphabet (same as in dataset)
    base_alphabet = (
        'abcdefghijklmnopqrstuvwxyz'
        'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
        '0123456789'
        '.,!?;: "#&\'()*+-/%=<>@[]^_`{|}~'
        'ƒÉ√¢√™√¥∆°∆∞ƒë'
        'ƒÇ√Ç√ä√î∆†∆Øƒê'
    )
    
    # Create converter and criterion
    converter = utils.CTCLabelConverter(base_alphabet)
    criterion = nn.CTCLoss(reduction='none', zero_infinity=True)
    
    try:
        # Encode base labels
        text, length = converter.encode(base_labels)
        
        # Pad diacritic sequences to same length
        max_diac_len = max(len(seq) for seq in diacritic_labels)
        padded_diacritic = []
        for seq in diacritic_labels:
            padded = seq + [0] * (max_diac_len - len(seq))
            padded_diacritic.append(padded)
        
        diacritic_targets = torch.tensor(padded_diacritic, dtype=torch.long)
        
        print(f"Text indices: {text}")
        print(f"Text lengths: {length}")
        print(f"Diacritic targets shape: {diacritic_targets.shape}")
        
        # Test forward pass with training mode
        model.train()
        base_logits, diacritic_logits = model(images, args.mask_ratio, args.max_span_length, use_masking=True)
        
        # Compute CTC loss
        batch_size = images.size(0)
        preds_size = torch.IntTensor([base_logits.size(1)] * batch_size)
        base_logits_ctc = base_logits.permute(1, 0, 2).log_softmax(2)
        loss_ctc = criterion(base_logits_ctc, text, preds_size, length).mean()
        
        # Compute diacritic loss
        # Need to align diacritic targets with model output sequence length
        batch_size, seq_len, _ = diacritic_logits.shape
        
        # Pad or truncate diacritic targets to match sequence length
        aligned_diacritic_targets = torch.zeros(batch_size, seq_len, dtype=torch.long)
        for b in range(batch_size):
            target_len = min(diacritic_targets.size(1), seq_len)
            aligned_diacritic_targets[b, :target_len] = diacritic_targets[b, :target_len]
        
        diacritic_logits_flat = diacritic_logits.view(-1, 6)
        diacritic_targets_flat = aligned_diacritic_targets.view(-1)
        loss_diac = nn.functional.cross_entropy(diacritic_logits_flat, diacritic_targets_flat)
        
        # Combined loss
        total_loss = args.alpha_ctc * loss_ctc + args.alpha_diac * loss_diac
        
        print(f"‚úÖ Loss computation successful!")
        print(f"CTC Loss: {loss_ctc.item():.4f}")
        print(f"Diacritic Loss: {loss_diac.item():.4f}")
        print(f"Total Loss: {total_loss.item():.4f}")
        
        return True, total_loss
        
    except Exception as e:
        print(f"‚ùå Loss computation failed: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def test_backward_pass(model, total_loss):
    """Test backward pass"""
    print("\nüîç Testing backward pass...")
    
    try:
        # Zero gradients
        model.zero_grad()
        
        # Backward pass
        total_loss.backward()
        
        # Check if gradients exist
        has_gradients = False
        grad_norm = 0.0
        param_count = 0
        
        for name, param in model.named_parameters():
            if param.grad is not None:
                has_gradients = True
                grad_norm += param.grad.norm().item()
                param_count += 1
        
        print(f"‚úÖ Backward pass successful!")
        print(f"Parameters with gradients: {param_count}")
        print(f"Total gradient norm: {grad_norm:.4f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Backward pass failed: {e}")
        return False

def test_inference_pipeline(model):
    """Test inference and text reconstruction"""
    print("\nüîç Testing inference pipeline...")
    
    try:
        # Create converter
        base_alphabet = (
            'abcdefghijklmnopqrstuvwxyz'
            'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
            '0123456789'
            '.,!?;: "#&\'()*+-/%=<>@[]^_`{|}~'
            'ƒÉ√¢√™√¥∆°∆∞ƒë'
            'ƒÇ√Ç√ä√î∆†∆Øƒê'
        )
        converter = utils.CTCLabelConverter(base_alphabet)
        
        # Create dummy image
        images = torch.randn(1, 1, 64, 512)
        
        model.eval()
        with torch.no_grad():
            base_logits, diacritic_logits = model(images)
            
            # Decode base characters
            base_logits_ctc = base_logits.permute(1, 0, 2).log_softmax(2)
            _, preds_index = base_logits_ctc.max(2)
            preds_index = preds_index.transpose(1, 0).contiguous().view(-1)
            preds_size = torch.IntTensor([base_logits.size(1)])
            base_pred = converter.decode(preds_index.data, preds_size.data)[0]
            
            # Decode diacritic predictions
            diacritic_pred = diacritic_logits.argmax(dim=-1).cpu().numpy()[0]
            
            # Merge to get final text
            diacritic_pred_list = diacritic_pred[:len(base_pred)].tolist()
            merged_text = merge_base_and_diacritic(base_pred, diacritic_pred_list)
            
            print(f"‚úÖ Inference pipeline successful!")
            print(f"Base prediction: '{base_pred}'")
            print(f"Diacritic prediction: {diacritic_pred_list}")
            print(f"Merged text: '{merged_text}'")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Inference pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_learning_step():
    """Test a simple learning step to see if loss decreases"""
    print("\nüîç Testing learning step...")
    
    # Create model and optimizer
    nb_cls = 84
    img_size = [512, 64]
    model = HTR_VT.create_model(nb_cls=nb_cls, img_size=img_size[::-1])
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    # Create fixed dummy data
    torch.manual_seed(42)
    images, base_labels, diacritic_labels = create_dummy_data(batch_size=2)
    
    # Setup
    base_alphabet = (
        'abcdefghijklmnopqrstuvwxyz'
        'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
        '0123456789'
        '.,!?;: "#&\'()*+-/%=<>@[]^_`{|}~'
        'ƒÉ√¢√™√¥∆°∆∞ƒë'
        'ƒÇ√Ç√ä√î∆†∆Øƒê'
    )
    converter = utils.CTCLabelConverter(base_alphabet)
    criterion = nn.CTCLoss(reduction='none', zero_infinity=True)
    
    class DummyArgs:
        mask_ratio = 0.0
        max_span_length = 1
        alpha_ctc = 1.0
        alpha_diac = 1.0
    
    args = DummyArgs()
    
    try:
        losses = []
        
        for step in range(5):
            # Forward pass
            model.train()
            base_logits, diacritic_logits = model(images, args.mask_ratio, args.max_span_length, use_masking=True)
            
            # Compute loss
            text, length = converter.encode(base_labels)
            batch_size = images.size(0)
            preds_size = torch.IntTensor([base_logits.size(1)] * batch_size)
            base_logits_ctc = base_logits.permute(1, 0, 2).log_softmax(2)
            loss_ctc = criterion(base_logits_ctc, text, preds_size, length).mean()
            
            # Pad diacritic sequences to match model output length
            batch_size, seq_len, _ = diacritic_logits.shape
            aligned_diacritic_targets = torch.zeros(batch_size, seq_len, dtype=torch.long)
            
            max_diac_len = max(len(seq) for seq in diacritic_labels)
            for b in range(batch_size):
                target_len = min(len(diacritic_labels[b]), seq_len)
                aligned_diacritic_targets[b, :target_len] = torch.tensor(diacritic_labels[b][:target_len])
            
            diacritic_logits_flat = diacritic_logits.view(-1, 6)
            diacritic_targets_flat = aligned_diacritic_targets.view(-1)
            loss_diac = nn.functional.cross_entropy(diacritic_logits_flat, diacritic_targets_flat)
            
            total_loss = args.alpha_ctc * loss_ctc + args.alpha_diac * loss_diac
            
            # Backward pass
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            losses.append(total_loss.item())
            print(f"Step {step+1}: Loss = {total_loss.item():.4f}")
        
        # Check if loss generally decreases
        if losses[-1] < losses[0]:
            print(f"‚úÖ Learning test successful! Loss decreased from {losses[0]:.4f} to {losses[-1]:.4f}")
            return True
        else:
            print(f"‚ö†Ô∏è  Loss didn't decrease much: {losses[0]:.4f} -> {losses[-1]:.4f} (this might be normal for very few steps)")
            return True  # Still consider it a pass since it didn't crash
            
    except Exception as e:
        print(f"‚ùå Learning test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("üöÄ Starting Multi-Task HTR-VT Quick Test")
    print("=" * 50)
    
    # Test 1: Forward pass
    success, model, images, base_labels, diacritic_labels = test_model_forward()
    if not success:
        print("üí• Stopping tests due to forward pass failure")
        return
    
    # Test 2: Loss computation
    success, total_loss = test_loss_computation(model, images, base_labels, diacritic_labels)
    if not success:
        print("üí• Stopping tests due to loss computation failure")
        return
    
    # Test 3: Backward pass
    success = test_backward_pass(model, total_loss)
    if not success:
        print("üí• Stopping tests due to backward pass failure")
        return
    
    # Test 4: Inference pipeline
    success = test_inference_pipeline(model)
    if not success:
        print("‚ö†Ô∏è  Inference test failed, but continuing...")
    
    # Test 5: Learning step
    success = test_learning_step()
    if not success:
        print("üí• Learning test failed")
        return
    
    print("\n" + "=" * 50)
    print("üéâ All tests passed! Your multi-task model is ready for training!")
    print("=" * 50)

if __name__ == '__main__':
    main()
